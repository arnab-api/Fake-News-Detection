{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "ubuntu_path = '/home/arnab/Desktop/Thesis Ubuntu/'\n",
    "windows_path = 'C:/Users/User/Desktop/Thesis_Windows/'\n",
    "\n",
    "root_path = windows_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-04 22:12:39,725 : INFO : loading Word2Vec object from C:/Users/User/Desktop/Thesis_Windows/W2V_D2V_models/model_2/kaler_kantha_W2V_10.model\n",
      "2018-09-04 22:12:41,898 : INFO : loading wv recursively from C:/Users/User/Desktop/Thesis_Windows/W2V_D2V_models/model_2/kaler_kantha_W2V_10.model.wv.* with mmap=None\n",
      "2018-09-04 22:12:41,898 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-09-04 22:12:41,898 : INFO : loading vocabulary recursively from C:/Users/User/Desktop/Thesis_Windows/W2V_D2V_models/model_2/kaler_kantha_W2V_10.model.vocabulary.* with mmap=None\n",
      "2018-09-04 22:12:41,898 : INFO : loading trainables recursively from C:/Users/User/Desktop/Thesis_Windows/W2V_D2V_models/model_2/kaler_kantha_W2V_10.model.trainables.* with mmap=None\n",
      "2018-09-04 22:12:41,906 : INFO : setting ignored attribute cum_table to None\n",
      "2018-09-04 22:12:41,910 : INFO : loaded C:/Users/User/Desktop/Thesis_Windows/W2V_D2V_models/model_2/kaler_kantha_W2V_10.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(root_path+\"W2V_D2V_models/model_2/kaler_kantha_W2V_10.model\")\n",
    "print(\"model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-04 22:12:42,697 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('গুরুদ্', 0.9302870035171509),\n",
       " ('জামায়াতশিবির', 0.9223557710647583),\n",
       " ('ফুটফাট', 0.9216894507408142),\n",
       " ('বিলশিমলা', 0.921260416507721),\n",
       " ('বিএনপিকর্মীদ', 0.9195979237556458),\n",
       " ('নারায়ণগঞ্', 0.9097241759300232),\n",
       " ('রেঁস্তোরা', 0.9079992175102234),\n",
       " ('গয়েশ্বর', 0.9057764410972595),\n",
       " ('হাসলিন্দা', 0.9040240049362183),\n",
       " ('দুগ্রুপ', 0.9028376936912537)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"নেতা\"\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7359494 ,  8.92657089,  5.53473663,  3.30629802,  2.48554087,\n",
       "        2.80652976, -4.22533369,  1.66060495, -3.79480195,  7.70339251], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[w1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[w1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1480\n",
      "{'headline': 'আদালত বোলা শাহাদ কুনো দুষ', 'body': '৫ নম্ব নারী শিশু নির্যাতন দমন ট্রাইবুনাল আদাল মহিলা বিচারক তানজি ইসমাঈল গৃহকর্মী নির্যাতন মামলা ক্রিকে শাহাদত হোস স্ত্রী জেসমিন জাহান কুনো দুষ খু পা বেকসু খালাস দিয় কার্য চলা কাল শাহাদত নিচ ছ দেখাল গা হাত তু আল্লাহ কসম শুন জাতী দল খেলা ফাষ্ট বোলা আম বাসা আ চুক্তি হ বাসা বোলিং পেক সম হেলপ তার বেটসম বানান যা বেটসগার্ল এক তার বোলিং সম বাউন্সা দিছিলাম মতন খ বল গিয়া লাগ চুখ পাশ মাননী আদালত আপন হেলমেট পই বেটিং নাম সেই দুষ আপন বোলার বল বেটসমান গা লাগা কারন মামলা হই ইতিহাস দুনিয়া কুথা ইতি পুর্ব স্ত্রী মারধ যৌতু মামলায় কারাগার কণ্ঠশিল্পী আরফিন রুমি জামিন রা মহিলা বিচারক তানজি ইসমাঈল ফরিয়াদি পক্ষ পশ্ন আসামী পক্ষ বলত এক কথা আপনা দা করত শিশু মেয়ে মা হই গা গাত তোলা হই আপনাদ দা স কুনো ভিডু আছে… কথা শেষ মামলা ডিশম শিশু নির্যাতন মামলা মহিলা বিচারপতি রা বেকসু খালাস পা শাহাদত মা আপ পছন্দ প্রশ্ন উত্তর বল জানি না…হা হা হা সবচা বড় কথা স্মার্টনেস অন স্মার্টনেস জান মান পোশাকআশাক চালচলন হাইট ইম্পর্টেন্ট স্মার্টনেস হাইট লম্ দেখব সম গুজা কুঁজো হ হাঁট স্মার্ট হ দরকা ও আছে… প্লেয় স দাড়ি থাকল আফ্রিদি আফ্রিদি লাগ বিদাশ দাড়ি থাকল বাংলাদেশি পাকিস্তানি… হাইট ইম্পরটেন্ট চেহা আল্লাহ রহম ভালো'}\n"
     ]
    }
   ],
   "source": [
    "with open(root_path + \"Dataset_preprocessed/News_Papers/motikontho_out_2.json\") as f:\n",
    "    fake_news = json.load(f)\n",
    "print(type(fake_news) , len(fake_news))\n",
    "print(fake_news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1480\n",
      "{'headline': 'মুক্তি পে ফিলিস্তিন প্রতিবাদ প্রতীক কিশোরী আহেদ', 'body': 'ইসরায়ে সেনা গাল চড় মার ফিলিস্তিন প্রতিবাদ প্রতীক হ ওঠা কারাগা ছাড়া পেয় আট মাস রোব কারাগা এক মুক্তি পান আহেদ মা এএফপি খবর তথ্য ১৭ বছর আহেদ মা সকাল ইসরায়াল শ্যারন কারাগা গাড়ি পশ্চিম তীর সীমান্তবর্তী তল্লাশি চৌকি পৌঁ আহাদ বাড়ি পশ্চিম তীর নবী সালেহ গ্রাম কারাগার মুখপাত্ আসাফ লিবরাতি ইসরায়ে সেনাদ হস্তান্ত স্বাগত জানা পরিবার সদস্য সমর্থক তল্লাশিচৌকি জড়ো হ বহনকারী সেনাবাহিনী গাড়ি দাঁড়ি পশ্চিম তীর যা সেনাদ মুক্তি পা রাস্ পাশ জড়ো হ হাজারো জন সাংবাদিকদ উদ্দাশ আহেদ সংবাদ সম্মেলন কথা আহাদ বা বাসেম এক হাত দি মা হা স্ত্রী রাখ পথ হাঁট চারপাশ স্বাধীন বাঁচ চা স্লোগান ওঠ ইসরায়ে কর্তৃপক্ষ গণমাধ্যম এড়ি এমন মামা তল্লাশি চৌ দি আ ধরন তথ্য দি প্রথম হ আহেদ মা পশ্চিম তীর ফিলিস্ শহ তুল্করাম শহর কা তল্লাশি চৌ দি আ তল্লাশি চৌ দি আ তিন তিন পরিবর্তন গত বছর শেষ মার্কিন প্রেসিডেন্ট ডোনাল্ড ট্রাম্প জেরুজালেম ইসরায়াল রাজধানী স্বীকৃতি দিল ক্ষোভ ফাট পড় ফিলিস্ মানুষ পবিত্ ভূমি জেরুজালেম রাজধানী হিসাব চা ফিলিস্তিনিদ প্রতিবাদ প্রতিবাদ থামা ইসরায়াল দমনপীড়ন মিলি সংঘা দাবানল ছড়ি পড় মধ্যপ্রাচ্য অঞ্চল যুক্তরাষ্ট্র সিদ্ধান্ত দশ ফিলিস্তিনি মান পারেনি কিশোরী আহেদ তামিমি পশ্চিম তীর নবী সালেহ গ্রাম বাড়ি ইসরায়াল সেনাদ নি সং পারেনি ক্ষোভ ফাট পড় সামাজিক যোগাযোগ ভাইরাল হ ভিডিও প্রথম মা ইসরায়ে সেনাদ যা চিৎকা সেনা সর কথাকাটাকা চল পক্ষ একপর্যা এক ইসরায়ে সেনা গাল চড় মার কিশোরী আহেদ বান্ধবী মোবাইল দৃশ্য ভিডি ভিডি ভাইরাল হ শিবির আগুন ধরা ফিলিস্তিনিদ দে সাহস ইসরায়েলিদ বিভ্রান্ত অপমানিত ক্রুদ্ধ লজ্জিত স্বয়ং ফিলিস্ প্রেসিডেন্ট মাহমুদ আব্বাস কিশোরী সাহ প্রশংসা সামাজিক যোগাযোগ সমর্থক প্রশংসা ভাসি দে বর্ণবাদী ১৬ বছর এক ফিলিস্ বালিকা স্পর্ধা ক্ষুব্ধ তিন ইসরায়ে চড় খা বিভ্রান্ত তিন দুর্বল দেখানো প্রত্যাহা রা আহেদ তামিমি এক চাচা ভা এন বন্দী কারাগার পর বাড়ি হা দি তছনছ মা পুলিশ স্টেশন মা নারিমান আল তামিমি গ্রেপ্ তিন পাঠানো কারাগার গত মার্চ মুক্তি পা আহাদ চাচা ভা পশ্চিম তীর সীমান্তবর্তী দেয়াল আহাদ ছ আঁকা কারণ গতকাল শনি ইসরায়ে কর্তৃপক্ষ ইতালিয়ান এক ফিলিস্ গ্রেপ্'}\n"
     ]
    }
   ],
   "source": [
    "with open(root_path + \"Dataset_preprocessed/News_Papers/real_news.json\") as f:\n",
    "    real_news = json.load(f)\n",
    "print(type(real_news) , len(real_news))\n",
    "print(real_news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2853 2853\n"
     ]
    }
   ],
   "source": [
    "def tokenize_a_news(news):\n",
    "    return news['body'].strip().split(' ')\n",
    "\n",
    "news_arr = []\n",
    "verdict = []\n",
    "for news in fake_news:\n",
    "    token = tokenize_a_news(news)\n",
    "    if(len(token) < 10):\n",
    "        continue\n",
    "    news_arr.append(news['body'])\n",
    "    verdict.append('FAKE')\n",
    "    \n",
    "for news in real_news:\n",
    "    token = tokenize_a_news(news)\n",
    "    if(len(token) < 10):\n",
    "        continue\n",
    "    news_arr.append(news['body'])\n",
    "    verdict.append('REAL')\n",
    "\n",
    "print(len(news_arr) , len(verdict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "news_train, news_test, verdict_train, verdict_test = train_test_split(news_arr, verdict, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TFIDF_W2V_merged_vectorizer:\n",
    "    def __init__(self , w2v_model , vector_sz , _max_df = 0.7 , _max_features = 2000):\n",
    "        self.w2v = w2v_model\n",
    "        self.vector_sz = vector_sz\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_df = _max_df , max_features = _max_features)\n",
    "    \n",
    "    def merge_with_word2vec(self , vector):\n",
    "        vector = vector.toarray()\n",
    "        ret = np.zeros((vector.shape[0] , vector.shape[1] , self.vector_sz , 2))\n",
    "        print(\"merging with word2vec :: vector shape => \" , vector.shape)\n",
    "        for d in range(vector.shape[0]):\n",
    "            if(d%100 == 0):\n",
    "                print('processing {} of {}'.format(d , vector.shape[0]))\n",
    "            for w in range(vector.shape[1]):\n",
    "                word = self.vocabulary[w]\n",
    "                tf_idf_val = vector[d][w]\n",
    "                \n",
    "                word_vec = np.ones(self.vector_sz)\n",
    "                try:\n",
    "                    word_vec = self.wv[word]\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                word_vec = tf_idf_val*word_vec\n",
    "                for idx in range(self.vector_sz):\n",
    "                    sign = 1\n",
    "                    if(word_vec[idx] < 0):\n",
    "                        sign = 2\n",
    "                    ret[d][w][idx][0] = sign\n",
    "                    ret[d][w][idx][0] = abs(word_vec[idx])\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    def fit_transform(self , doc_arr):\n",
    "        vector = self.tfidf_vectorizer.fit_transform(doc_arr)\n",
    "        self.vocabulary = self.tfidf_vectorizer.get_feature_names()\n",
    "        return self.merge_with_word2vec(vector)\n",
    "    \n",
    "    def transform(self , doc_arr):\n",
    "        vector = self.tfidf_vectorizer.transform(doc_arr)\n",
    "        return self.merge_with_word2vec(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, with little normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging with word2vec :: vector shape =>  (1911, 2000)\n",
      "processing 0 of 1911\n",
      "processing 100 of 1911\n",
      "processing 200 of 1911\n",
      "processing 300 of 1911\n",
      "processing 400 of 1911\n",
      "processing 500 of 1911\n",
      "processing 600 of 1911\n",
      "processing 700 of 1911\n",
      "processing 800 of 1911\n",
      "processing 900 of 1911\n",
      "processing 1000 of 1911\n",
      "processing 1100 of 1911\n",
      "processing 1200 of 1911\n",
      "processing 1300 of 1911\n",
      "processing 1400 of 1911\n",
      "processing 1500 of 1911\n",
      "processing 1600 of 1911\n",
      "processing 1700 of 1911\n",
      "processing 1800 of 1911\n",
      "processing 1900 of 1911\n",
      "merging with word2vec :: vector shape =>  (942, 2000)\n",
      "processing 0 of 942\n",
      "processing 100 of 942\n",
      "processing 200 of 942\n",
      "processing 300 of 942\n",
      "processing 400 of 942\n",
      "processing 500 of 942\n",
      "processing 600 of 942\n",
      "processing 700 of 942\n",
      "processing 800 of 942\n",
      "processing 900 of 942\n",
      "vector ready\n"
     ]
    }
   ],
   "source": [
    "vector_size = 10\n",
    "vectorizer = TFIDF_W2V_merged_vectorizer(model , vector_size)\n",
    "vector_train = vectorizer.fit_transform(news_train)\n",
    "vector_test = vectorizer.transform(news_test)\n",
    "print(\"vector ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1911, 2000, 10, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(942, 2000, 10, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics\n",
    "print('import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(verdict_train)\n",
    "y_test = le.transform(verdict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(2000,10,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',metrics.binary_accuracy])\n",
    "    print('compile done')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "k.set_image_dim_ordering('tf')\n",
    "\n",
    "def transfer_learning_1():\n",
    "    model = VGG16(weights='imagenet', include_top=False,input_shape = (2000, 10, 2))\n",
    "\n",
    "  # use transfer learning for re-training the last layers\n",
    "  # Freeze first 25 layers, so that we can retrain 26th and so on using our classes.\n",
    "    for layer in model.layers[:-5]:\n",
    "        layer.trainable = False\n",
    "\n",
    "  # Adding our new layers \n",
    "    top_layers = model.output\n",
    "    top_layers = Flatten(input_shape=model.output_shape[1:])(top_layers)\n",
    "    top_layers = Dense(num_of_classes, activation=\"relu\",input_shape=(num_classes,))(top_layers)\n",
    "    top_layers = Dropout(0.5)(top_layers)\n",
    "    top_layers = Dense(num_of_classes, activation=\"relu\",input_shape=(num_classes,))(top_layers)\n",
    "    top_layers = Dense(num_of_classes, activation=\"softmax\")(top_layers)\n",
    "\n",
    "  # Add top layers on top of freezed (not re-trained) layers of VGG16\n",
    "    model_final = Model(input = model.input, output = top_layers)\n",
    "\n",
    "  # Compile the model\n",
    "    model_final.summary()\n",
    "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_train = vector_train.reshape(vector_train.shape[0] , vector_train.shape[1]*vector_train.shape[2]*vector_train.shape[3])\n",
    "# vector_test = vector_test.reshape(vector_test.shape[0] , vector_test.shape[1]*vector_test.shape[2]*vector_test.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1911, 2000, 10, 2) (942, 2000, 10, 2)\n",
      "(1911,) (942,)\n"
     ]
    }
   ],
   "source": [
    "print(vector_train.shape , vector_test.shape)\n",
    "print(y_train.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input must have 3 channels; got `input_shape=(2000, 10, 2)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-8fef9fd74588>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model = get_simple_model()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransfer_learning_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_test\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-d4c8660fe1a2>\u001b[0m in \u001b[0;36mtransfer_learning_1\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtransfer_learning_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m   \u001b[1;31m# use transfer learning for re-training the last layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\applications\\vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[1;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[0;32m    100\u001b[0m                                       \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                                       \u001b[0mrequire_flatten\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                                       weights=weights)\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\applications\\imagenet_utils.py\u001b[0m in \u001b[0;36m_obtain_input_shape\u001b[1;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     raise ValueError('The input must have 3 channels; got '\n\u001b[1;32m--> 173\u001b[1;33m                                      '`input_shape=' + str(input_shape) + '`')\n\u001b[0m\u001b[0;32m    174\u001b[0m                 if ((input_shape[0] is not None and input_shape[0] < min_size) or\n\u001b[0;32m    175\u001b[0m                    (input_shape[1] is not None and input_shape[1] < min_size)):\n",
      "\u001b[1;31mValueError\u001b[0m: The input must have 3 channels; got `input_shape=(2000, 10, 2)`"
     ]
    }
   ],
   "source": [
    "# model = get_simple_model()\n",
    "model = transfer_learning_1()\n",
    "model.fit(vector_train,y_train,batch_size=2,epochs=10,verbose=1,validation_data=(vector_test , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
