{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "ubuntu_path = '/home/arnab/Desktop/Thesis Ubuntu/'\n",
    "windows_path = 'C:/Users/User/Desktop/Thesis_Windows/'\n",
    "\n",
    "root_path = windows_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-19 20:02:59,264 : INFO : loading Word2Vec object from C:/Users/User/Desktop/Thesis_Windows/W2V_D2V_models/model_2/kaler_kantha_W2V_10.model\n",
      "2018-08-19 20:03:01,163 : INFO : loading wv recursively from C:/Users/User/Desktop/Thesis_Windows/W2V_D2V_models/model_2/kaler_kantha_W2V_10.model.wv.* with mmap=None\n",
      "2018-08-19 20:03:01,163 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-08-19 20:03:01,163 : INFO : loading vocabulary recursively from C:/Users/User/Desktop/Thesis_Windows/W2V_D2V_models/model_2/kaler_kantha_W2V_10.model.vocabulary.* with mmap=None\n",
      "2018-08-19 20:03:01,175 : INFO : loading trainables recursively from C:/Users/User/Desktop/Thesis_Windows/W2V_D2V_models/model_2/kaler_kantha_W2V_10.model.trainables.* with mmap=None\n",
      "2018-08-19 20:03:01,175 : INFO : setting ignored attribute cum_table to None\n",
      "2018-08-19 20:03:01,179 : INFO : loaded C:/Users/User/Desktop/Thesis_Windows/W2V_D2V_models/model_2/kaler_kantha_W2V_10.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(root_path+\"W2V_D2V_models/model_2/kaler_kantha_W2V_10.model\")\n",
    "print(\"model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-19 20:03:04,948 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('গুরুদ্', 0.9302870035171509),\n",
       " ('জামায়াতশিবির', 0.9223557710647583),\n",
       " ('ফুটফাট', 0.9216894507408142),\n",
       " ('বিলশিমলা', 0.921260416507721),\n",
       " ('বিএনপিকর্মীদ', 0.9195979237556458),\n",
       " ('নারায়ণগঞ্', 0.9097241759300232),\n",
       " ('রেঁস্তোরা', 0.9079992175102234),\n",
       " ('গয়েশ্বর', 0.9057764410972595),\n",
       " ('হাসলিন্দা', 0.9040240049362183),\n",
       " ('দুগ্রুপ', 0.9028376936912537)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"নেতা\"\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7359494 ,  8.92657089,  5.53473663,  3.30629802,  2.48554087,\n",
       "        2.80652976, -4.22533369,  1.66060495, -3.79480195,  7.70339251], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[w1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[w1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1480\n",
      "{'headline': 'আদালত বোলা শাহাদ কুনো দুষ', 'body': '৫ নম্ব নারী শিশু নির্যাতন দমন ট্রাইবুনাল আদাল মহিলা বিচারক তানজি ইসমাঈল গৃহকর্মী নির্যাতন মামলা ক্রিকে শাহাদত হোস স্ত্রী জেসমিন জাহান কুনো দুষ খু পা বেকসু খালাস দিয় কার্য চলা কাল শাহাদত নিচ ছ দেখাল গা হাত তু আল্লাহ কসম শুন জাতী দল খেলা ফাষ্ট বোলা আম বাসা আ চুক্তি হ বাসা বোলিং পেক সম হেলপ তার বেটসম বানান যা বেটসগার্ল এক তার বোলিং সম বাউন্সা দিছিলাম মতন খ বল গিয়া লাগ চুখ পাশ মাননী আদালত আপন হেলমেট পই বেটিং নাম সেই দুষ আপন বোলার বল বেটসমান গা লাগা কারন মামলা হই ইতিহাস দুনিয়া কুথা ইতি পুর্ব স্ত্রী মারধ যৌতু মামলায় কারাগার কণ্ঠশিল্পী আরফিন রুমি জামিন রা মহিলা বিচারক তানজি ইসমাঈল ফরিয়াদি পক্ষ পশ্ন আসামী পক্ষ বলত এক কথা আপনা দা করত শিশু মেয়ে মা হই গা গাত তোলা হই আপনাদ দা স কুনো ভিডু আছে… কথা শেষ মামলা ডিশম শিশু নির্যাতন মামলা মহিলা বিচারপতি রা বেকসু খালাস পা শাহাদত মা আপ পছন্দ প্রশ্ন উত্তর বল জানি না…হা হা হা সবচা বড় কথা স্মার্টনেস অন স্মার্টনেস জান মান পোশাকআশাক চালচলন হাইট ইম্পর্টেন্ট স্মার্টনেস হাইট লম্ দেখব সম গুজা কুঁজো হ হাঁট স্মার্ট হ দরকা ও আছে… প্লেয় স দাড়ি থাকল আফ্রিদি আফ্রিদি লাগ বিদাশ দাড়ি থাকল বাংলাদেশি পাকিস্তানি… হাইট ইম্পরটেন্ট চেহা আল্লাহ রহম ভালো'}\n"
     ]
    }
   ],
   "source": [
    "with open(root_path + \"Dataset_preprocessed/News_Papers/motikontho_out_2.json\") as f:\n",
    "    fake_news = json.load(f)\n",
    "print(type(fake_news) , len(fake_news))\n",
    "print(fake_news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1480\n",
      "{'headline': 'মুক্তি পে ফিলিস্তিন প্রতিবাদ প্রতীক কিশোরী আহেদ', 'body': 'ইসরায়ে সেনা গাল চড় মার ফিলিস্তিন প্রতিবাদ প্রতীক হ ওঠা কারাগা ছাড়া পেয় আট মাস রোব কারাগা এক মুক্তি পান আহেদ মা এএফপি খবর তথ্য ১৭ বছর আহেদ মা সকাল ইসরায়াল শ্যারন কারাগা গাড়ি পশ্চিম তীর সীমান্তবর্তী তল্লাশি চৌকি পৌঁ আহাদ বাড়ি পশ্চিম তীর নবী সালেহ গ্রাম কারাগার মুখপাত্ আসাফ লিবরাতি ইসরায়ে সেনাদ হস্তান্ত স্বাগত জানা পরিবার সদস্য সমর্থক তল্লাশিচৌকি জড়ো হ বহনকারী সেনাবাহিনী গাড়ি দাঁড়ি পশ্চিম তীর যা সেনাদ মুক্তি পা রাস্ পাশ জড়ো হ হাজারো জন সাংবাদিকদ উদ্দাশ আহেদ সংবাদ সম্মেলন কথা আহাদ বা বাসেম এক হাত দি মা হা স্ত্রী রাখ পথ হাঁট চারপাশ স্বাধীন বাঁচ চা স্লোগান ওঠ ইসরায়ে কর্তৃপক্ষ গণমাধ্যম এড়ি এমন মামা তল্লাশি চৌ দি আ ধরন তথ্য দি প্রথম হ আহেদ মা পশ্চিম তীর ফিলিস্ শহ তুল্করাম শহর কা তল্লাশি চৌ দি আ তল্লাশি চৌ দি আ তিন তিন পরিবর্তন গত বছর শেষ মার্কিন প্রেসিডেন্ট ডোনাল্ড ট্রাম্প জেরুজালেম ইসরায়াল রাজধানী স্বীকৃতি দিল ক্ষোভ ফাট পড় ফিলিস্ মানুষ পবিত্ ভূমি জেরুজালেম রাজধানী হিসাব চা ফিলিস্তিনিদ প্রতিবাদ প্রতিবাদ থামা ইসরায়াল দমনপীড়ন মিলি সংঘা দাবানল ছড়ি পড় মধ্যপ্রাচ্য অঞ্চল যুক্তরাষ্ট্র সিদ্ধান্ত দশ ফিলিস্তিনি মান পারেনি কিশোরী আহেদ তামিমি পশ্চিম তীর নবী সালেহ গ্রাম বাড়ি ইসরায়াল সেনাদ নি সং পারেনি ক্ষোভ ফাট পড় সামাজিক যোগাযোগ ভাইরাল হ ভিডিও প্রথম মা ইসরায়ে সেনাদ যা চিৎকা সেনা সর কথাকাটাকা চল পক্ষ একপর্যা এক ইসরায়ে সেনা গাল চড় মার কিশোরী আহেদ বান্ধবী মোবাইল দৃশ্য ভিডি ভিডি ভাইরাল হ শিবির আগুন ধরা ফিলিস্তিনিদ দে সাহস ইসরায়েলিদ বিভ্রান্ত অপমানিত ক্রুদ্ধ লজ্জিত স্বয়ং ফিলিস্ প্রেসিডেন্ট মাহমুদ আব্বাস কিশোরী সাহ প্রশংসা সামাজিক যোগাযোগ সমর্থক প্রশংসা ভাসি দে বর্ণবাদী ১৬ বছর এক ফিলিস্ বালিকা স্পর্ধা ক্ষুব্ধ তিন ইসরায়ে চড় খা বিভ্রান্ত তিন দুর্বল দেখানো প্রত্যাহা রা আহেদ তামিমি এক চাচা ভা এন বন্দী কারাগার পর বাড়ি হা দি তছনছ মা পুলিশ স্টেশন মা নারিমান আল তামিমি গ্রেপ্ তিন পাঠানো কারাগার গত মার্চ মুক্তি পা আহাদ চাচা ভা পশ্চিম তীর সীমান্তবর্তী দেয়াল আহাদ ছ আঁকা কারণ গতকাল শনি ইসরায়ে কর্তৃপক্ষ ইতালিয়ান এক ফিলিস্ গ্রেপ্'}\n"
     ]
    }
   ],
   "source": [
    "with open(root_path + \"Dataset_preprocessed/News_Papers/real_news.json\") as f:\n",
    "    real_news = json.load(f)\n",
    "print(type(real_news) , len(real_news))\n",
    "print(real_news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2853 2853\n"
     ]
    }
   ],
   "source": [
    "def tokenize_a_news(news):\n",
    "    return news['body'].strip().split(' ')\n",
    "\n",
    "news_arr = []\n",
    "verdict = []\n",
    "for news in fake_news:\n",
    "    token = tokenize_a_news(news)\n",
    "    if(len(token) < 10):\n",
    "        continue\n",
    "    news_arr.append(news['body'])\n",
    "    verdict.append('FAKE')\n",
    "    \n",
    "for news in real_news:\n",
    "    token = tokenize_a_news(news)\n",
    "    if(len(token) < 10):\n",
    "        continue\n",
    "    news_arr.append(news['body'])\n",
    "    verdict.append('REAL')\n",
    "\n",
    "print(len(news_arr) , len(verdict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "news_train, news_test, verdict_train, verdict_test = train_test_split(news_arr, verdict, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class TFIDF_W2V_merged_vectorizer:\n",
    "    def __init__(self , w2v_model , vector_sz , _max_df = 0.7 , _max_features = 2000):\n",
    "        self.w2v = w2v_model\n",
    "        self.vector_sz = vector_sz\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_df = _max_df , max_features = _max_features)\n",
    "    \n",
    "    def merge_with_word2vec(self , vector):\n",
    "        vector = vector.toarray()\n",
    "        ret = np.zeros((vector.shape[0] , vector.shape[1] , self.vector_sz , 2))\n",
    "        print(\"merging with word2vec :: vector shape => \" , vector.shape)\n",
    "        for d in range(vector.shape[0]):\n",
    "            if(d%100 == 0):\n",
    "                print('processing {} of {}'.format(d , vector.shape[0]))\n",
    "            for w in range(vector.shape[1]):\n",
    "                word = self.vocabulary[w]\n",
    "                tf_idf_val = vector[d][w]\n",
    "                \n",
    "                word_vec = np.ones(self.vector_sz)\n",
    "                try:\n",
    "                    word_vec = self.wv[word]\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                word_vec = tf_idf_val*word_vec\n",
    "                for idx in range(self.vector_sz):\n",
    "                    sign = 1\n",
    "                    if(word_vec[idx] < 0):\n",
    "                        sign = 2\n",
    "                    ret[d][w][idx][0] = sign\n",
    "                    ret[d][w][idx][0] = abs(word_vec[idx])\n",
    "        \n",
    "        return ret\n",
    "    \n",
    "    def fit_transform(self , doc_arr):\n",
    "        vector = self.tfidf_vectorizer.fit_transform(doc_arr)\n",
    "        self.vocabulary = self.tfidf_vectorizer.get_feature_names()\n",
    "        return self.merge_with_word2vec(vector)\n",
    "    \n",
    "    def transform(self , doc_arr):\n",
    "        vector = self.tfidf_vectorizer.transform(doc_arr)\n",
    "        return self.merge_with_word2vec(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, with little normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging with word2vec :: vector shape =>  (1911, 2000)\n",
      "processing 0 of 1911\n",
      "processing 100 of 1911\n",
      "processing 200 of 1911\n",
      "processing 300 of 1911\n",
      "processing 400 of 1911\n",
      "processing 500 of 1911\n",
      "processing 600 of 1911\n",
      "processing 700 of 1911\n",
      "processing 800 of 1911\n",
      "processing 900 of 1911\n",
      "processing 1000 of 1911\n",
      "processing 1100 of 1911\n",
      "processing 1200 of 1911\n",
      "processing 1300 of 1911\n",
      "processing 1400 of 1911\n",
      "processing 1500 of 1911\n",
      "processing 1600 of 1911\n",
      "processing 1700 of 1911\n",
      "processing 1800 of 1911\n",
      "processing 1900 of 1911\n",
      "merging with word2vec :: vector shape =>  (942, 2000)\n",
      "processing 0 of 942\n",
      "processing 100 of 942\n",
      "processing 200 of 942\n",
      "processing 300 of 942\n",
      "processing 400 of 942\n",
      "processing 500 of 942\n",
      "processing 600 of 942\n",
      "processing 700 of 942\n",
      "processing 800 of 942\n",
      "processing 900 of 942\n",
      "vector ready\n"
     ]
    }
   ],
   "source": [
    "vector_size = 10\n",
    "vectorizer = TFIDF_W2V_merged_vectorizer(model , vector_size)\n",
    "vector_train = vectorizer.fit_transform(news_train)\n",
    "vector_test = vectorizer.transform(news_test)\n",
    "print(\"vector ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1911, 2000, 10, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(942, 2000, 10, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vector_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics\n",
    "print('import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(verdict_train)\n",
    "y_test = le.transform(verdict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(2000,10,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',metrics.binary_accuracy])\n",
    "    print('compile done')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_train = vector_train.reshape(vector_train.shape[0] , vector_train.shape[1]*vector_train.shape[2]*vector_train.shape[3])\n",
    "# vector_test = vector_test.reshape(vector_test.shape[0] , vector_test.shape[1]*vector_test.shape[2]*vector_test.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1911, 2000, 10, 2) (942, 2000, 10, 2)\n",
      "(1911,) (942,)\n"
     ]
    }
   ],
   "source": [
    "print(vector_train.shape , vector_test.shape)\n",
    "print(y_train.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 2000, 10, 512)     1536      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 2000, 10, 512)     0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2000, 10, 256)     131328    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 2000, 10, 256)     0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 5120000)           0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 5120001   \n",
      "=================================================================\n",
      "Total params: 5,252,865\n",
      "Trainable params: 5,252,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n",
      "Train on 1911 samples, validate on 942 samples\n",
      "Epoch 1/10\n",
      "1911/1911 [==============================] - 1401s 733ms/step - loss: 0.2217 - acc: 0.9173 - binary_accuracy: 0.9173 - val_loss: 0.1581 - val_acc: 0.9533 - val_binary_accuracy: 0.9533\n",
      "Epoch 2/10\n",
      "1911/1911 [==============================] - 1399s 732ms/step - loss: 0.0368 - acc: 0.9890 - binary_accuracy: 0.9890 - val_loss: 0.1830 - val_acc: 0.9533 - val_binary_accuracy: 0.9533\n",
      "Epoch 3/10\n",
      "1911/1911 [==============================] - 1398s 732ms/step - loss: 0.0072 - acc: 0.9969 - binary_accuracy: 0.9969 - val_loss: 0.3500 - val_acc: 0.9522 - val_binary_accuracy: 0.9522\n",
      "Epoch 4/10\n",
      " 136/1911 [=>............................] - ETA: 20:32 - loss: 3.6727e-04 - acc: 1.0000 - binary_accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-900a059ce121>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_simple_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_test\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2352\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = get_simple_model()\n",
    "model.fit(vector_train,y_train,batch_size=2,epochs=10,verbose=1,validation_data=(vector_test , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
